# Suppress python warnings about deprecations
import warnings
warnings.simplefilter("ignore")

# Imports
import os
import sys
import keras
import cv2 as cv
import numpy as np
import matplotlib.pyplot as plt

from classifiers import CNN
from bcdr import Reader as bcdr_reader
from keras.preprocessing.image import ImageDataGenerator

# OpenCV an Tensorflow versions
print("OpenCV Version: {}".format(cv.__version__))
print("Keras Version: {}\n".format(keras.__version__))

# Check BCDR instance to use
current_path = os.getcwd()
if len(sys.argv) == 1:
    print('\n[WARNING] Using BCDR default instance ({})'.format('BCDR-F01'))
    dataset_path = os.path.join(current_path, 'BCDR-F01')
else:
    if os.path.isdir(sys.argv[1]):
        dataset_path = os.path.join(current_path, sys.argv[1])
    else:
        print('\n[ERROR] The is no instance available of the BCDR dataset with that name.')
        print('\nAvailable instances:')
        dirs = [d for d in os.listdir(os.getcwd()) if os.path.isdir(d)]
        for dir in dirs:
            print('\t- {}'.format(dir))
        sys.exit()

batch_size = 32
num_classes = 2
epochs = 100
num_predictions = 20

# The data, shuffled and split between train and test sets:
(x_train, y_train), (x_test, y_test) = bcdr_reader(dataset_path,
                                                   'outlines.csv',
                                                   num_classes).load_data()

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model2 = CNN(num_classes, x_train[0].shape).createModel()

# initiate RMSprop optimizer
opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)

# Let's train the model using RMSprop
model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy','loss'])

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255

# This will do preprocessing and realtime data augmentation:
datagen = ImageDataGenerator(
    featurewise_center=False,  # set input mean to 0 over the dataset
    samplewise_center=False,  # set each sample mean to 0
    featurewise_std_normalization=False,  # divide inputs by std of the dataset
    samplewise_std_normalization=False,  # divide each input by its std
    zca_whitening=False,  # apply ZCA whitening
    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
    horizontal_flip=True,  # randomly flip images
    vertical_flip=False)  # randomly flip images

# Compute quantities required for feature-wise normalization
# (std, mean, and principal components if ZCA whitening is applied).
datagen.fit(x_train)

# Fit the model on the batches generated by datagen.flow().
history = model.fit_generator(datagen.flow(x_train, y_train,
                                 batch_size=batch_size),
                              epochs=epochs,
                              validation_data=(x_test, y_test),
                              workers=4)

# Score trained model.
scores = model.evaluate(x_test, y_test, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])

# Loss Curves
plt.figure(figsize=[8,6])
plt.plot(history.history['loss'],'r',linewidth=3.0)
plt.plot(history.history['val_loss'],'b',linewidth=3.0)
plt.legend(['Training loss', 'Validation Loss'],fontsize=18)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.title('Loss Curves',fontsize=16)

# Accuracy Curves
plt.figure(figsize=[8,6])
plt.plot(history.history['acc'],'r',linewidth=3.0)
plt.plot(history.history['val_acc'],'b',linewidth=3.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Accuracy',fontsize=16)
plt.title('Accuracy Curves',fontsize=16)
